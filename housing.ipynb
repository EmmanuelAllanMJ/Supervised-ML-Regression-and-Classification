{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0,2.0])   #(size in 1000 square feet)\n",
    "y_train = np.array([300.0,500.0])   #(price in 1000s of dollars)\n",
    "plt.plot(x_train,y_train);\n",
    "plt.scatter(x_train,y_train,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(0,m):\n",
    "        f_wb = w*x[i]+b\n",
    "        cost_sum += (f_wb - y[i])**2\n",
    "    total_cost = (1/(2*m)) * cost_sum\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(x_train,y_train,340,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.,500.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        f_wb = w*x[i]+b\n",
    "        dj_dw+=(f_wb - y[i])*x[i]\n",
    "        dj_db += (f_wb - y[i])\n",
    "    dj_dw = (1/m) * dj_dw\n",
    "    dj_db = (1/m) * dj_db\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "   \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w_in,b_in,alpha,num_iter,compute_gradient):\n",
    "    m = x.shape[0]\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(0,num_iter):\n",
    "        dj_dw,dj_db = compute_gradient(x,y,w,b)\n",
    "        w = w-alpha*dj_dw\n",
    "        b = b-alpha*dj_db\n",
    "    return w,b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 0\n",
    "b_init = 0\n",
    "iteration = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "w_final, b_final = gradient_descent(x_train,y_train,w_init,b_init,tmp_alpha,iteration,compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        f_wb = w*x[i]+b\n",
    "        dj_dw += (f_wb-y[i])*x[i]\n",
    "        dj_db += (f_wb-y[i])\n",
    "    dj_dw /=m\n",
    "    dj_db /=m\n",
    "    return dj_dw,dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w_init,b_init,alpha,num_iter,cost_function,compute_gradient):\n",
    "    m = x.shape[0]\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    \n",
    "    for i in range(0,num_iter):\n",
    "        dj_dw, dj_db = compute_gradient(x,y,w,b)\n",
    "        w = w-alpha * dj_dw\n",
    "        b = b-alpha * dj_db\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iter/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                    f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                    f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 0.001\n",
    "# run gradient descent\n",
    "x_train = np.array([])\n",
    "y_train = np.array([10,19 ,28,37,50,63,70 ,85,90,100])\n",
    "for i in range (0,10):\n",
    "    x_train = np.append(x_train,i)   \n",
    "    # y_train = np.append(y_train,i*100+50)   \n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=x_train,y = y_train)\n",
    "x_final = np.array([])\n",
    "y_final = np.array([])\n",
    "for i in (0,10):\n",
    "    x_final = np.append(x_final,i)\n",
    "    y_final = np.append(y_final,w_final*i+b_final)\n",
    "plt.plot(x_final,y_final,'r--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_value = 3\n",
    "y_value = w_final*x_value+b_final\n",
    "print(y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLotting the result\n",
    "plt.scatter(x=x_train,y = y_train)\n",
    "plt.plot(x_final,y_final,'r--');\n",
    "plt.scatter(x = x_value,y = y_value, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have more than one feature \n",
    "# x = size, no. of bedroom, age; y = price\n",
    "np.set_printoptions(precision=2) \n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X,y,w,b):\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb =np.dot(w,X[i])+b\n",
    "        cost +=(f_wb-y[i])**2\n",
    "    cost = cost/(2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test case for cost function\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "cost = compute_cost(X_train,y_train,w_init,b_init)\n",
    "print(f'Cost of optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent with multiple variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,w,b):\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    for i in range (m):\n",
    "        err = (np.dot(X[i],w)+b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j]+err*X[i,j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    \n",
    "    return dj_db,dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,w_in, b_in, cost_function, gradient_function, alpha,num_iters):\n",
    "    # m = X.shape[0]\n",
    "    w = copy.deepcopy(w_in)\n",
    "    J_history = []\n",
    "    \n",
    "    b = b_in\n",
    "    for i in range(num_iters):\n",
    "        dj_db, dj_dw = gradient_function(X,y,w,b)\n",
    "        w = w- alpha*dj_dw\n",
    "        b = b- alpha*dj_db\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:,:4]\n",
    "    y = data[:,4]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_house_data()\n",
    "print(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    X_norm = (X-mu)/sigma\n",
    "    return (X_norm, mu,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_norm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.zeros_like(w_init)\n",
    "b_init = 0\n",
    "alpha = 1.0e-1\n",
    "nums_iter = 1000\n",
    "w_final, b_final,J_hist = gradient_descent(X_norm,y_train,w_init,b_init,compute_cost,compute_gradient,alpha,nums_iter)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_final) + b_final\n",
    "    \n",
    "    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter( X_train[:,i],y_train,label='target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],yp, label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend();\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, normalize out example.\n",
    "x_house = np.array([1200, 3, 1, 40])\n",
    "x_house_norm = (x_house - X_mu) / X_sigma\n",
    "print(x_house_norm)\n",
    "x_house_predict = np.dot(x_house_norm, w_final) + b_final\n",
    "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-Learn for scaling regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "print(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale/normalize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X_norm = scalar.fit_transform(X_train)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")\n",
    "print( \"model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using sgdr.predict()\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "# make a prediction using w,b. \n",
    "y_pred = np.dot(X_norm, w_norm) + b_norm  \n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],y_pred,label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend();\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "X_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300, 500])   #target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "#X must be a 2d matrix\n",
    "linear_model.fit(X_train.reshape(-1,1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")\n",
    "print(f\"'manual' prediction: f_wb = wx+b : {1200*w + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_model.predict(X_train.reshape(-1, 1))\n",
    "\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "\n",
    "X_test = np.array([[1200]])\n",
    "print(f\"Prediction for 1200 sqft house: ${linear_model.predict(X_test)[0]:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using multiple features\n",
    "# load the dataset\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction on training set:\\n {linear_model.predict(X_train)[:4]}\" )\n",
    "print(f\"prediction using w,b:\\n {(X_train @ w + b)[:4]}\")\n",
    "print(f\"Target values \\n {y_train[:4]}\")\n",
    "\n",
    "x_house = np.array([1200, 3,1, 40]).reshape(-1,4)\n",
    "x_house_predict = linear_model.predict(x_house)[0]\n",
    "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using new housing csv with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading housing price\n",
    "hp = pd.read_csv('./housing.csv')\n",
    "hp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = ['longitude','latitude','housing_median_age','total_rooms','population','households','median_income']\n",
    "X_train = hp[X_features]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = hp['median_house_value']\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X_norm = scalar.fit_transform(X_train)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDRegressor(max_iter = 1000)\n",
    "sgdr = sgdr.fit(X_norm,y_train)\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "y_pred = np.dot(X_norm, w_norm) + b_norm  \n",
    "\n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "r2_score = regressor.score(X_train,y_train)\n",
    "print(r2_score*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting target and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig,ax=plt.subplots(1,7,figsize=(12,3),sharey=True)\n",
    "print(ax)\n",
    "for i in range(len(ax)):\n",
    "    # sns.scatterplot(x=X_features[i],y='median_house_value', data=hp)\n",
    "    # ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].scatter(X_train[:][X_features[i]],y_pred,label = 'predict')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:][X_features[i]],y_train,label = 'target')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend();\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1])\n",
    "X_train2 = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train2 = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = y_train == 1\n",
    "neg = y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "#plot 1, single variable\n",
    "ax[0].scatter(x_train[pos], y_train[pos], marker='x', s=100, c = 'red', label=\"y=1\")\n",
    "ax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", c='blue', \n",
    "              lw=3)\n",
    "\n",
    "ax[0].set_ylim(-0.08,1.1)\n",
    "ax[0].set_ylabel('y', fontsize=12)\n",
    "ax[0].set_xlabel('x', fontsize=12)\n",
    "ax[0].set_title('one variable plot')\n",
    "ax[0].legend()\n",
    "\n",
    "#plot 2, two variables\n",
    "# plot_data(X_train2, y_train2, ax[1])\n",
    "ax[1].scatter(X_train2[pos], y_train2[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax[1].scatter(X_train2[neg], y_train2[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
    "              lw=3)\n",
    "ax[1].axis([0, 4, 0, 4])\n",
    "ax[1].set_ylabel('$x_1$', fontsize=12)\n",
    "ax[1].set_xlabel('$x_0$', fontsize=12)\n",
    "ax[1].set_title('two variable plot')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "#X must be a 2d matrix\n",
    "linear_model.fit(x_train.reshape(-1,1),y_train)\n",
    "\n",
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")\n",
    "print(f\"'manual' prediction: f_wb = wx+b : {1200*w + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,3))\n",
    "#plot 1, single variable\n",
    "plt.scatter(x_train[pos], y_train[pos], marker='x', s=100, c = 'red', label=\"y=1\")\n",
    "plt.scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", c='blue', \n",
    "              lw=3)\n",
    "\n",
    "l = []\n",
    "for i in range(len(x_train)):\n",
    "    l.append(x_train[i]*w + b)\n",
    "\n",
    "plt.plot(l,y_train)\n",
    "# plt.set_ylim(-0.08,1.1)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.title('One variable plot')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(z):\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of evenly spaced values between -10 and 10\n",
    "z_tmp = np.arange(-10,11)\n",
    "\n",
    "# Use the function implemented above to get the sigmoid values\n",
    "y = sigmoid(z_tmp)\n",
    "\n",
    "# Code for pretty printing the two arrays next to each other\n",
    "np.set_printoptions(precision=3) \n",
    "print(\"Input (z), Output (sigmoid(z))\")\n",
    "print(np.c_[z_tmp, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot z vs sigmoid(z)\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,3))\n",
    "ax.plot(z_tmp, y, c=\"b\")\n",
    "\n",
    "ax.set_title(\"Sigmoid function\")\n",
    "ax.set_ylabel('sigmoid(z)')\n",
    "ax.set_xlabel('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1])\n",
    "\n",
    "w_in = np.zeros((1))\n",
    "b_in = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "def plot_logistic_1d(X,y):\n",
    "    pos = y_train == 1\n",
    "    neg = y_train == 0\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "    #plot 1, single variable\n",
    "    ax.scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "    ax.scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
    "                edgecolors=\"blue\",lw=3)\n",
    "\n",
    "    ax.set_ylim(-0.08,1.1)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_title('one variable plot')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "def plot_logistic_2d(X,y):\n",
    "    fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n",
    "    neg = neg.reshape(-1,)\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s = 80, c = 'red', label=\"y=1\")\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s = 80, label=\"y=0\", facecolors='none', edgecolors='blue', lw=3)\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    ax.axis([0, 4, 0, 3.5])\n",
    "    ax.set_ylabel('$x_1$')\n",
    "    ax.set_xlabel('$x_0$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sigmoid(z):\n",
    "    fig,ax = plt.subplots(1,1,figsize=(5,3))\n",
    "    # Plot z vs sigmoid(z)\n",
    "    ax.plot(z, sigmoid(z), c=\"b\")\n",
    "\n",
    "    ax.set_title(\"Sigmoid function\")\n",
    "    ax.set_ylabel('sigmoid(z)')\n",
    "    ax.set_xlabel('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x_train,y_train)\n",
    "# plt.plot()\n",
    "plot_logistic_1d(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1]).reshape(-1,1) \n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logistic_2d(X,y)\n",
    "def f(x,w,b):\n",
    "    return w*x+b\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "x = np.linspace(-10,10,100)\n",
    "ax.plot(x,f(x,-3,3),color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_sigmoid(np.arange(-10,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X,y,w,b):\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w)+b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "    cost = cost/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])     \n",
    "w_tmp = np.array([1,1])\n",
    "b_tmp = -3\n",
    "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n",
    "    \"\"\" plots logistic data with two axis \"\"\"\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n",
    "    neg = neg.reshape(-1,)\n",
    "\n",
    "    # Plot examples\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors='blue', lw=3)\n",
    "    ax.legend(loc=loc)\n",
    "\n",
    "    ax.figure.canvas.toolbar_visible = False\n",
    "    ax.figure.canvas.header_visible = False\n",
    "    ax.figure.canvas.footer_visible = False\n",
    "\n",
    "# Choose values between 0 and 6\n",
    "x0 = np.arange(0,6)\n",
    "\n",
    "# Plot the two decision boundaries\n",
    "x1 = 3 - x0\n",
    "x1_other = 4 - x0\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "# Plot the decision boundary\n",
    "ax.plot(x0,x1, c='blue', label=\"$b$=-3\")\n",
    "ax.plot(x0,x1_other, c='red', label=\"$b$=-4\")\n",
    "ax.axis([0, 4, 0, 4])\n",
    "\n",
    "# Plot the original data\n",
    "plot_data(X_train,y_train,ax)\n",
    "ax.axis([0, 4, 0, 4])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_array1 = np.array([1,1])\n",
    "b_1 = -3\n",
    "w_array2 = np.array([1,1])\n",
    "b_2 = -4\n",
    "\n",
    "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
    "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X,y,w,b):\n",
    "    m = X.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w)+b;\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        dj_dw += (f_wb_i-y[i])*X[i]\n",
    "        dj_db += (f_wb_i-y[i])\n",
    "    dj_db /=m\n",
    "    dj_dw /=m\n",
    "    return dj_db,dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,w_in,b_in,alpha,num_iters):\n",
    "    m = X.shape[0]\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_db , dj_dw = compute_gradient_logistic(X,y,w,b)\n",
    "        w = w-alpha*dj_dw\n",
    "        b = b-alpha*dj_db\n",
    "                # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n",
    "    \"\"\" plots logistic data with two axis \"\"\"\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n",
    "    neg = neg.reshape(-1,)\n",
    "\n",
    "    # Plot examples\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors='blue', lw=3)\n",
    "    ax.legend(loc=loc)\n",
    "\n",
    "    ax.figure.canvas.toolbar_visible = False\n",
    "    ax.figure.canvas.header_visible = False\n",
    "    ax.figure.canvas.footer_visible = False\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "# plot the probability \n",
    "# plt_prob(ax, w_out, b_out)\n",
    "\n",
    "# Plot the original data\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')   \n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "plot_data(X_train,y_train,ax)\n",
    "\n",
    "# Plot the decision boundary\n",
    "x0 = -b_out/w_out[0]\n",
    "x1 = -b_out/w_out[1]\n",
    "ax.plot([0,x0],[x1,0], c='blue', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 feature\n",
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1])\n",
    "\n",
    "w_tmp  = np.zeros_like(x_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")\n",
    "\n",
    "plot_logistic_1d(x_train,y_train)\n",
    "plt.plot([0,3],[b_out,3*w_out+b_out])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9912f26da2b08db387aec33622e23a94541d2b78bc1c67b181d2791965f4c8ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
